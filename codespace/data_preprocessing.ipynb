{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "對新聞、討論的文章進行資料前處理，包含斷詞、去除標點符號、計算 tf-idf、特徵選取等等，以利後續的模型訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 CKIP Tagger 進行斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "2024-04-13 16:15:52.142556: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# 先把我們需要的函數載入\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "ws = WS(\"./data_ckip\") # 斷詞\n",
    "pos = POS(\"./data_ckip\") # 詞性標注\n",
    "ner = NER(\"./data_ckip\") # 命名實體識別 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入資料集\n",
    "import pandas as pd\n",
    "data_dir = '../data/'\n",
    "\n",
    "df_news = pd.read_csv(data_dir + 'news_filtered_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>foreign_investor_surplus</th>\n",
       "      <th>investment_trust_surplus</th>\n",
       "      <th>dealer_surplus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-01 00:00:00</td>\n",
       "      <td>40895704</td>\n",
       "      <td>6247000</td>\n",
       "      <td>13420418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-02 00:00:00</td>\n",
       "      <td>-58004507</td>\n",
       "      <td>5013000</td>\n",
       "      <td>8992716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-03 00:00:00</td>\n",
       "      <td>45348028</td>\n",
       "      <td>3661000</td>\n",
       "      <td>25176165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-04 00:00:00</td>\n",
       "      <td>-865155</td>\n",
       "      <td>14308000</td>\n",
       "      <td>-5958158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-07 00:00:00</td>\n",
       "      <td>-62514759</td>\n",
       "      <td>2651000</td>\n",
       "      <td>-21795347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  foreign_investor_surplus  investment_trust_surplus   \n",
       "0  2022-03-01 00:00:00                  40895704                   6247000  \\\n",
       "1  2022-03-02 00:00:00                 -58004507                   5013000   \n",
       "2  2022-03-03 00:00:00                  45348028                   3661000   \n",
       "3  2022-03-04 00:00:00                   -865155                  14308000   \n",
       "4  2022-03-07 00:00:00                 -62514759                   2651000   \n",
       "\n",
       "   dealer_surplus  \n",
       "0        13420418  \n",
       "1         8992716  \n",
       "2        25176165  \n",
       "3        -5958158  \n",
       "4       -21795347  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chip = pd.read_csv(data_dir + '籌碼數據-2年_by_date.csv')\n",
    "df_chip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge news and chip data by date\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])\n",
    "df_chip['date'] = pd.to_datetime(df_chip['date'])\n",
    "# merge the data by date in df_chip and post_time in df_news\n",
    "df = pd.merge(df_news, df_chip, how='inner', on='date')\n",
    "# drop column 'Unnamed: 0'\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged data\n",
    "df.to_csv(data_dir + 'news_filtered_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移除被標註為「持平」的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 3\n",
    "\n",
    "# 把標籤為 -1 的 row 全部移除\n",
    "df_news = df_news[df_news['label_day' + str(days)] != -1]\n",
    "Y = get_label(df_news, days) # 取得標籤\n",
    "\n",
    "# 提取正文內容\n",
    "corpus = df_news['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行斷詞\n",
    "word_sentence_list = ws(corpus, \n",
    "                        sentence_segmentation=True,\n",
    "                        segment_delimiter_set={'?', '？', '!', '！', '。', ',','，', ';', ':', '、', ' ', '.'})\n",
    "# 移除標點符號\n",
    "punc = ['，', '。', '、', '：', '；', '？', '！', '「', '」', '（', '）', '『', '』', '—', '－', '～', '…', '‧', '《', '》', '〈', '〉', '﹏﹏']\n",
    "eng_punc = [',', '.', ':', ';', '?', '!', '(', ')', '[', ']', '&', '@', '#', '$', '%', '-', '_', '*', '/', '\\\\', '+', '=', '>', '<', '\"', \"'\", '’', '‘', '“', '”', ' ']\n",
    "stop_words = ['全文', '日', '月', '年', 'br', '中央社', '公司', '上午', '下午', '日期'] # 停用詞\n",
    "\n",
    "# 只要詞裡面含有數字就移除\n",
    "word_sentence_list = [[word for word in sentence if not any(char.isdigit() for char in word)] for sentence in word_sentence_list]\n",
    "word_sentence_list = [[word for word in sentence if word not in punc] for sentence in word_sentence_list]\n",
    "word_sentence_list = [[word for word in sentence if word not in eng_punc] for sentence in word_sentence_list]\n",
    "word_sentence_list = [[word for word in sentence if word not in stop_words] for sentence in word_sentence_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得 n-gram (n = 1 ~ 3) 的 BOW Count\n",
    "最後會被存在名為 `bow_count` 的 scipy.sparse._csr.csr_matrix 物件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 923189)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(ngram_range=(1, 3))\n",
    "bow_count = bow.fit_transform([' '.join(sentence) for sentence in word_sentence_list])\n",
    "print(bow_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得 n-gram (n = 1 ~ 3) 的 tf-idf\n",
    "最後會被存在名為 `tfidf` 的 scipy.sparse._csr.csr_matrix 物件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 923189)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tv = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tfidf = tv.fit_transform([' '.join(sentence) for sentence in word_sentence_list])\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square 特徵選取\n",
    "根據標籤內容，計算每個詞的 chi-square 值，並選取前 1000 個詞作為特徵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 選取前 1000 個特徵\n",
    "ch2 = SelectKBest(chi2, k=1000)\n",
    "X_chi2 = ch2.fit_transform(tfidf, Y)\n",
    "print(X_chi2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['出席' '電芯' '早餐' '中信 投顧' '長榮 國際 單位' '台塑 智能' '私募' '上市 中櫃 單位' '中櫃 單位' '上市 中櫃'\n",
      " '換約 漲幅' 'jtb' '瑞銀' '張紹豐' '鮮食' '上市 新興' '新興 單位' '上市 新興 單位' '發行 長榮 國際' '火箭'\n",
      " '減資案' '取消' '速報 中航' '盤中 速報 中航' '彭士孝' '旅展' '現金 減資' '先生' '鄭文燦' '年度 分派 股利'\n",
      " '年度 分派' '買回' '協商' '減資' '中航' '修訂案' '外站' '參加' '證券 舉辦 法說會' '江耀宗' '台股 企業日'\n",
      " '企業日' '義大利麵' '元富 證券 舉辦' '長家' '衰退率' '買賣 br' '列車' '參加 元富' '參加 元富 證券' '副駕駛'\n",
      " 'br 盤中 速報' 'br 盤中' '配送' '全家' '盤中 速報' '租回' '爭議' '上市 亞航' '上市 亞航 單位' '亞航 單位'\n",
      " '台灣 高鐵' '閱讀 時報 台北' '太空' '時報 資訊' '速報' '閱讀 時報' '大榮' '嘉里 大榮' '工會' '盈餘 閱讀 時報'\n",
      " '上市 單位' '盈餘 閱讀' 'br' '罷工' '上市 裕民' '投控 單位' '上市 台驊' '上市 台驊 投控' '台驊 投控 單位'\n",
      " '資訊 航運股' '時報 資訊 航運股' '裕民 單位' '上市 裕民 單位' '機師 工會' '嘉里' '閱讀 上市' '年增 閱讀 上市'\n",
      " '上市' '高鐵' '顏益財' '上市 長榮' '單位' '長榮 單位' '上市 長榮 單位' '財經 公告' '營收 年增 閱讀'\n",
      " '年增 閱讀' '閱讀' '機師']\n"
     ]
    }
   ],
   "source": [
    "# 顯示 X_chi2 的特徵名稱\n",
    "feature_names = bow.get_feature_names_out()\n",
    "mask = ch2.get_support() # 這個 mask 會告訴你哪些特徵被選取了\n",
    "selected_features = feature_names[mask]\n",
    "# 從 selected features 中隨機選取 10 個特徵\n",
    "# 取出 chi2 分數最高的 100 個特徵\n",
    "top_100 = np.argsort(ch2.scores_)[-100:]\n",
    "selected_features = feature_names[top_100]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 儲存處理好的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X_chi2\n",
    "from scipy import sparse\n",
    "sparse.save_npz(data_dir + 'X_chi2.npz', X_chi2)\n",
    "sparse.save_npz(data_dir + 'Y.npz', sparse.csr_matrix(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 函數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segmentation(corpus, ws):\n",
    "    # 進行斷詞\n",
    "    word_sentence_list = ws(corpus, \n",
    "                            sentence_segmentation=True,\n",
    "                            segment_delimiter_set={'?', '？', '!', '！', '。', ',','，', ';', ':', '、', ' ', '.'})\n",
    "    # 移除標點符號\n",
    "    punc = ['，', '。', '、', '：', '；', '？', '！', '「', '」', '（', '）', '『', '』', '—', '－', '～', '…', '‧', '《', '》', '〈', '〉', '﹏﹏']\n",
    "    eng_punc = [',', '.', ':', ';', '?', '!', '(', ')', '[', ']', '&', '@', '#', '$', '%', '-', '_', '*', '/', '\\\\', '+', '=', '>', '<', '\"', \"'\", '’', '‘', '“', '”', ' ']\n",
    "    stop_words = ['全文', '日', '月', '年', 'br', '中央社', '公司', '上午', '下午', '日期'] # 停用詞\n",
    "\n",
    "    # 只要詞裡面含有數字就移除\n",
    "    word_sentence_list = [[word for word in sentence if not any(char.isdigit() for char in word)] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in punc] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in eng_punc] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in stop_words] for sentence in word_sentence_list]\n",
    "        \n",
    "    return word_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_text, Y, ws):\n",
    "    corpus = df_text['content'].tolist()\n",
    "    word_sentence_list = word_segmentation(corpus, ws)\n",
    "    tv = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    tfidf = tv.fit_transform([' '.join(sentence) for sentence in word_sentence_list])\n",
    "    # Chi2 feature selection\n",
    "    ch2 = SelectKBest(chi2, k=1000)\n",
    "    X_chi2 = ch2.fit_transform(tfidf, Y)\n",
    "    # Add foreign_investor_surplus, investment_trust_surplus, and dealer_surplus to X_chi2\n",
    "    X_chi2 = sparse.hstack((X_chi2, sparse.csr_matrix(df_text[['foreign_investor_surplus', 'investment_trust_surplus', 'dealer_surplus']])))\n",
    "    return X_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df_text \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_filtered_merged.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m get_label(df_text, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36mpreprocessing\u001b[0;34m(df_text, Y, ws)\u001b[0m\n\u001b[1;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m df_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m word_sentence_list \u001b[38;5;241m=\u001b[39m word_segmentation(corpus, ws)\n\u001b[0;32m----> 4\u001b[0m tv \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      5\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m tv\u001b[38;5;241m.\u001b[39mfit_transform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m word_sentence_list])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Chi2 feature selection\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "\n",
    "df_text = pd.read_csv(data_dir + 'news_filtered_merged.csv')\n",
    "Y = get_label(df_text, 3)\n",
    "X = preprocessing(df_text, Y, ws)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
