{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengliang/anaconda3/lib/python3.11/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Users/chengliang/anaconda3/lib/python3.11/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Users/chengliang/anaconda3/lib/python3.11/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import sparse\n",
    "\n",
    "# 先把我們需要的函數載入\n",
    "ws = WS(\"./data_ckip\") # 斷詞\n",
    "pos = POS(\"./data_ckip\") # 詞性標注\n",
    "ner = NER(\"./data_ckip\") # 命名實體識別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segmentation(contents, ws):\n",
    "    word_sentence_list = ws(contents, \n",
    "                        sentence_segmentation=True,\n",
    "                        segment_delimiter_set={'?', '？', '!', '！', '。', ',','，', ';', ':', '、', ' ', '.'})\n",
    "    # 標點符號\n",
    "    punc = ['，', '。', '、', '：', '；', '？', '！', '「', '」', '（', '）', '『', '』', '—', '－', '～', '…', '‧', '《', '》', '〈', '〉', '﹏﹏']\n",
    "    eng_punc = [',', '.', ':', ';', '?', '!', '(', ')', '[', ']', '&', '@', '#', '$', '%', '-', '_', '*', '/', '\\\\', '+', '=', '>', '<', '\"', \"'\", '’', '‘', '“', '”', ' ']\n",
    "    # 停用詞\n",
    "    stop_words = ['全文', '日', '月', '年', 'br', '中央社', '公司', '上午', '下午', '日期']\n",
    "\n",
    "    word_sentence_list = [[word for word in sentence if not any(char.isdigit() for char in word)] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in punc] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in eng_punc] for sentence in word_sentence_list]\n",
    "    word_sentence_list = [[word for word in sentence if word not in stop_words] for sentence in word_sentence_list]\n",
    "\n",
    "    return word_sentence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, days, ws):\n",
    "    df = df[df['label_day' + str(days)] != -1] # 把標籤為 -1 的 row 全部移除\n",
    "    Y = lambda df, days: df['label_day' + str(days)].tolist() # 提取標籤\n",
    "    contents = df['content'].tolist() # 提取正文內容\n",
    "    # 對正文內容進行斷詞\n",
    "    word_sentence_list = word_segmentation(contents, ws)\n",
    "    # 取得 1-gram - 3-gram 的 tf-idf 特徵\n",
    "    tv = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    tfidf = tv.fit_transform([' '.join(sentence) for sentence in word_sentence_list])\n",
    "    # 取得前 1000 個特徵\n",
    "    ch2 = SelectKBest(chi2, k=1000)\n",
    "    X = ch2.fit_transform(tfidf, Y(df, days))\n",
    "    # 將 foreign_investor_surplus, investment_trust_surplus, dealer_surplus 加入到 X\n",
    "    X = sparse.hstack((X, sparse.csr_matrix(df[['foreign_investor_surplus', 'investment_trust_surplus', 'dealer_surplus']])))\n",
    "    return X, Y(df, days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('../data/news_filtered_merged.csv')\n",
    "\n",
    "# 將日期欄位轉換為日期時間類型\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# 定義每個時段的長度（以天為單位）\n",
    "period_length = 90  # 90天為一個時段\n",
    "\n",
    "# 初始化結果列表，用於存放每個時段的預測結果和準確率\n",
    "results = []\n",
    "\n",
    "# 初始化混淆矩陣\n",
    "total_conf_matrix = np.zeros((2, 2))\n",
    "\n",
    "# 設定起始日期為資料中最小日期\n",
    "start_date = data['date'].min()\n",
    "\n",
    "# 限制測試次數用\n",
    "i = 0\n",
    "\n",
    "\n",
    "# 分段處理資料\n",
    "with tqdm(total = 9) as pbar:\n",
    "    while start_date + pd.Timedelta(days=period_length - 1) <= data['date'].max():\n",
    "        \n",
    "        # 取得該時段的結束日期\n",
    "        end_date = start_date + pd.Timedelta(days=period_length - 1)\n",
    "        \n",
    "        # 取出該時段的資料\n",
    "        period_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "        \n",
    "        # 準備特徵和標籤\n",
    "        X, y = preprocessing(period_data, 1, ws)\n",
    "\n",
    "        y = np.ravel(np.array(y))\n",
    "        X = X.toarray()\n",
    "\n",
    "        # 切分資料為訓練集和測試集（按照時間順序）\n",
    "        split_index = int(len(X) * 0.8)  # 取前80%作為訓練集\n",
    "        X_train, X_test = X[:split_index], X[split_index:]\n",
    "        y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "        with open(\"model.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # 訓練模型\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 進行預測\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # 計算準確率\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # 計算混淆矩陣\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # 累加混淆矩陣\n",
    "        total_conf_matrix += conf_matrix\n",
    "        \n",
    "        # 記錄結果\n",
    "        results.append({'Start_Date': start_date, 'End_Date': end_date, 'Accuracy': accuracy})\n",
    "\n",
    "        # 更新起始日期為下一個時段的起始日期\n",
    "        start_date = start_date + pd.Timedelta(days=period_length)\n",
    "        pbar.update(1)\n",
    "\n",
    "# 輸出結果\n",
    "for result in results:\n",
    "    print(\"Period Start:\", result['Start_Date'], \"Period End:\", result['End_Date'], \"Accuracy:\", result['Accuracy'])\n",
    "\n",
    "# 計算總準確率\n",
    "total_accuracy = sum(result['Accuracy'] for result in results) / len(results)\n",
    "print(\"Total accuracy:\", total_accuracy)\n",
    "\n",
    "# 輸出總混淆矩陣\n",
    "print(\"Total Confusion Matrix:\")\n",
    "print(total_conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
